spark.master yarn
spark.submit.deployMode client
spark.yarn.jars=local:/usr/lib/spark/jars/*
spark.eventLog.enabled true
spark.eventLog.dir hdfs://sparktest-m/user/spark/eventlog

# Dynamic allocation on YARN
spark.dynamicAllocation.enabled true
spark.dynamicAllocation.minExecutors 1
spark.executor.instances 10000
spark.dynamicAllocation.maxExecutors 10000
spark.shuffle.service.enabled true
spark.scheduler.minRegisteredResourcesRatio 0.0

spark.yarn.historyServer.address sparktest-m:18080
spark.history.fs.logDirectory hdfs://sparktest-m/user/spark/eventlog

# These configs should all be overridden by the server but exist here as a
# fallback
spark.executor.cores 2
spark.executor.memory 4655m
# Overkill
spark.yarn.am.memory 4655m

spark.driver.memory 3760m
spark.driver.maxResultSize 1880m
spark.rpc.message.maxSize 512

spark.driver.extraJavaOptions -Dflogger.backend_factory=com.google.cloud.hadoop.repackaged.gcs.com.google.common.flogger.backend.log4j.Log4jBackendFactory#getInstance
spark.executor.extraJavaOptions -Dflogger.backend_factory=com.google.cloud.hadoop.repackaged.gcs.com.google.common.flogger.backend.log4j.Log4jBackendFactory#getInstance

# Disable Parquet metadata caching as its URI re-encoding logic does
# not work for GCS URIs (b/28306549). The net effect of this is that
# Parquet metadata will be read both driver side and executor side.
spark.sql.parquet.cacheMetadata=false

# This undoes setting hive.execution.engine to tez in hive-site.xml
# It is not used by Spark
spark.hadoop.hive.execution.engine=mr

# User-supplied properties.
#Fri Dec 28 11:16:00 UTC 2018
spark.scheduler.mode=FAIR
spark.sql.cbo.enabled=true
spark.executorEnv.OPENBLAS_NUM_THREADS=1
spark.executor.instances=2
spark.executor.memory=5586m
spark.driver.memory=3840m
spark.executor.cores=2
spark.yarn.am.memory=640m
spark.driver.maxResultSize=1920m

spark.mongodb.output.uri=mongodb://10.150.0.3/
spark.mongodb.output.database=newdb123
spark.mongodb.output.collection=rating
spark.mongodb.keep_alive_ms=10000
